{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2579c-34e1-4637-a708-3ea744bce6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"torch==2.2.1\" tensorboard \n",
    "\n",
    "!pip install  -q --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\" \"optuna\" \"scikit-learn\"\n",
    "\n",
    "!huggingface-cli login --token \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305243f-0ad6-44c1-b4d7-e5d3fe6cb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai\n",
    "!pip install -q accelerate evaluate\n",
    "import torch\n",
    "import evaluate\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936bb9f-1c2e-4470-ac0c-67193215f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resuse https://github.com/pkasela/DESIRE-ME/blob/main/src/model/utils.py\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    logger.info(f'Setting global random seed to {seed}')\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae2aa1-e507-4100-9edc-975683d7e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857625d5-4fbc-4fdd-8826-060e984b80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def gpt_generation_topic():\n",
    "    client = OpenAI(api_key = \"...\")\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      #model=\"gpt-3.5-turbo\",\n",
    "      messages = [\n",
    "            #{\n",
    "               # \"role\": \"system\",\n",
    "               #\"content\": \"Please come up with a topic, and based on the topic, generate a specific query and an ambiguous query. The output format should be: Topic: {topic}\\n Specific query: {specific query}\\n Ambiguous query: {ambiguous query}\"\n",
    "            #},\n",
    "            {\"role\": \"user\", \"content\": \"Please come up with a topic, and based on the topic, generate a specific query and an ambiguous query. The output format should be: Topic: {topic}\\n Specific query: {specific query}\\n Ambiguous query: {ambiguous query}\"}],\n",
    "      max_tokens=512,\n",
    "      top_p=1,\n",
    "      temperature=1.0\n",
    "    )\n",
    "\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a143a9-13fa-4354-b501-4ed0a4ea5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ace69f-5f31-4a46-b45c-025aef23efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_topic_generation():\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Follow the instruction step by step. 1. Come up with 10 topics. 2. Based on the topics, generate 10 corresponding user information needs. 3. Based on the user information needs, generate 10 corresponding specific queries. 4. Based on the user information needs, generate 10 corresponding ambiguous queries. The output format should be Topic: {topic}\\n User information need: {user information need}\\n Specific query: {specific query}\\n Ambiguous query: {ambiguous query}\"},\n",
    "    ]\n",
    "    \n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=1.0,\n",
    "        do_sample=True\n",
    "    \n",
    "    )\n",
    "    #print(outputs[0][\"generated_text\"][-1]['content'])\n",
    "    return outputs[0][\"generated_text\"][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee505e92-7036-4225-846b-6a0ade4ff93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435a3b6-4884-4fd3-ac5c-4b11d875a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi3_generation_topic():\n",
    "     \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Please come up with a topic, and based on the topic, generate a specific query and an ambiguous query. The output format should be: Topic: {topic}\\n Specific query: {specific query}\\n Ambiguous query: {ambiguous query}\"},    \n",
    "    ]\n",
    "\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 1.0,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "    \n",
    "    output = pipe(messages, **generation_args)\n",
    "    #print(output[0]['generated_text'])\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09b686-d44c-4b44-8805-2bcdc16bd220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f81a7d-a19e-490f-95bb-8e62c1f4daf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
